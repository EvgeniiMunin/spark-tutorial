{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib: Basic Statistics and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective is to play with a small dataset from the Hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mllib-tutorial.ipynb           Spark_correction_tutorial.ipynb\r\n",
      "mllib-tutorial-students.ipynb  Spark_tutorial_students.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import Row\n",
    "\n",
    "n_examples = 1000\n",
    "n_feats=11\n",
    "\n",
    "def gen_features(_):\n",
    "    return np.random.randn(1, n_feats).flatten().tolist()\n",
    "\n",
    "def gen_dict(x): \n",
    "    return dict([ (\"feature_{}\".format(i) , e) for i, e in enumerate(x) ])\n",
    "\n",
    "featuresDF = sc.parallelize(range(n_examples)).map(gen_features).map(gen_dict).map(lambda r: Row(**r))\n",
    "\n",
    "featuresDF=sqlContext.createDataFrame(featuresDF)\n",
    "\n",
    "featuresDF.write.parquet(\"/home/matthieu_le_goff/spark-tutorial/input-dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2286099587270376, 0.5083651052766873, -0.5272028734422457, -0.23747419008827422, -1.6039189103925688, 0.7295034651296943, 0.24573184837021395, 1.4005658579060023, -1.0406163559216033, 0.1758052835612565, -0.3064545508872768]\n"
     ]
    }
   ],
   "source": [
    "featuresDF = sc.parallelize(range(n_examples)).map(gen_features)\n",
    "print(featuresDF.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresDF=None\n",
    "featuresDF = sqlContext.read.parquet(\"/home/matthieu_le_goff/spark-tutorial/input-dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, parsing the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- feature_0: double (nullable = true)\n",
      " |-- feature_1: double (nullable = true)\n",
      " |-- feature_10: double (nullable = true)\n",
      " |-- feature_2: double (nullable = true)\n",
      " |-- feature_3: double (nullable = true)\n",
      " |-- feature_4: double (nullable = true)\n",
      " |-- feature_5: double (nullable = true)\n",
      " |-- feature_6: double (nullable = true)\n",
      " |-- feature_7: double (nullable = true)\n",
      " |-- feature_8: double (nullable = true)\n",
      " |-- feature_9: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = featuresDF.describe().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Statistics count\n",
      "feature_10: 1000\n",
      "summary: count\n",
      "feature_8: 1000\n",
      "feature_9: 1000\n",
      "feature_2: 1000\n",
      "feature_3: 1000\n",
      "feature_0: 1000\n",
      "feature_1: 1000\n",
      "feature_6: 1000\n",
      "feature_7: 1000\n",
      "feature_4: 1000\n",
      "feature_5: 1000\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics mean\n",
      "feature_10: 0.021629714868542822\n",
      "summary: mean\n",
      "feature_8: 0.03411087104521261\n",
      "feature_9: 0.04760686252662131\n",
      "feature_2: -0.01651924887134106\n",
      "feature_3: 0.004302160644344671\n",
      "feature_0: -0.05458669093655795\n",
      "feature_1: 0.07591372627210322\n",
      "feature_6: 0.016071453810657506\n",
      "feature_7: -0.047463444974409456\n",
      "feature_4: -0.040010827075252296\n",
      "feature_5: 0.009890709057433654\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics stddev\n",
      "feature_10: 0.9825420772692703\n",
      "summary: stddev\n",
      "feature_8: 1.0186248346945252\n",
      "feature_9: 1.0119930348969068\n",
      "feature_2: 0.9601107948692916\n",
      "feature_3: 1.018268338009346\n",
      "feature_0: 1.0719935478003724\n",
      "feature_1: 0.9948617364436604\n",
      "feature_6: 1.0040374446267113\n",
      "feature_7: 1.0025740499898157\n",
      "feature_4: 1.0350545528044437\n",
      "feature_5: 1.0225090520288114\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics min\n",
      "feature_10: -3.8309734720093855\n",
      "summary: min\n",
      "feature_8: -2.346531751662909\n",
      "feature_9: -2.73675561873232\n",
      "feature_2: -2.9704439342020823\n",
      "feature_3: -3.592765029661747\n",
      "feature_0: -3.361227154770821\n",
      "feature_1: -3.9701732588051644\n",
      "feature_6: -2.7710798951885764\n",
      "feature_7: -2.3607377812859247\n",
      "feature_4: -2.863455394758099\n",
      "feature_5: -3.1958735724030336\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics max\n",
      "feature_10: 2.556889723539157\n",
      "summary: max\n",
      "feature_8: 3.0604706129969115\n",
      "feature_9: 2.880603813112414\n",
      "feature_2: 2.6176646521541236\n",
      "feature_3: 2.713937497163865\n",
      "feature_0: 2.580497558079707\n",
      "feature_1: 3.243495152201929\n",
      "feature_6: 3.212773133742057\n",
      "feature_7: 3.1382876814669762\n",
      "feature_4: 3.0605994248259223\n",
      "feature_5: 3.631713435722846\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in result:\n",
    "    print \"----------------------\"\n",
    "    r = l.asDict()\n",
    "    print \"Statistics {}\".format(r[\"summary\"])\n",
    "    for key in r.keys():\n",
    "        print \"{0}: {1}\".format(key, r[key])\n",
    "    print \"----------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "label_df = featuresDF.withColumn(\"label\", (featuresDF.feature_1>0.1).cast(\"int\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute statistics by labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  502|\n",
      "|    0|  498|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df.select(\"label\").groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with Apache Spark\n",
    "Now that the inputs are defined, we can apply some basics (or advanced) data processing functions to classify the type of interactions (i.e. \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "col_names=featuresDF.columns\n",
    "s = StringIndexer(inputCol=\"label\", outputCol=\"idx_label\").fit(label_df.select(col_names + [\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = s.transform(label_df.select(col_names + [\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = label_df.select(col_names + [\"label\"]).randomSplit([0.6,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy on both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|prediction|idx_label|count|\n",
      "+----------+---------+-----+\n",
      "|       1.0|      1.0|  198|\n",
      "|       0.0|      1.0|    2|\n",
      "|       0.0|      0.0|  205|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "preds = model.transform(test)\n",
    "print preds.where(preds.prediction == preds.idx_label).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try applying a PCA before learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, pca, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|prediction|idx_label|count|\n",
      "+----------+---------+-----+\n",
      "|       1.0|      1.0|  163|\n",
      "|       0.0|      1.0|   37|\n",
      "|       1.0|      0.0|  165|\n",
      "|       0.0|      0.0|   40|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try applying a kmeans to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=2, seed=1, featuresCol=\"features\", predictionCol=\"kmeans_pred\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "kmeans_assemblor = VectorAssembler(inputCols=col_names+[\"kmeans_pred\"], outputCol=\"kmeans_features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"kmeans_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, kmeans, kmeans_assemblor, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, pca, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(pca.k, [2, 5, 8]) \\\n",
    "    .addGrid(rf.maxDepth, [1, 10]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|prediction|idx_label|count|\n",
      "+----------+---------+-----+\n",
      "|       1.0|      1.0|  163|\n",
      "|       0.0|      1.0|   37|\n",
      "|       1.0|      0.0|  165|\n",
      "|       0.0|      0.0|   40|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercices:\n",
    "* Compare these models to the ones in scikit learn\n",
    "* Detect classification noise using multiple models\n",
    "    - If different models keep giving wrong predictions on the same sample, it may be a labeling mistake\n",
    "    \n",
    "Optional but good for training:\n",
    "* Implement the Viola Jones strategy using Spark\n",
    "    - Lear a classifier (Decision Tree)\n",
    "    - Reduce to zero the FPR using a threshold on probability\n",
    "    - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
