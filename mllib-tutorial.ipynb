{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib: Basic Statistics and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in our first notebook, we will use the reduced dataset provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million network interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, parsing the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # keep just numeric and logical values\n",
    "    symbolic_indexes = [1,2,3,41]\n",
    "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
    "    return [float(x) for x in clean_line_split]\n",
    "\n",
    "vector_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\n",
    "             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
    "             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
    "             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "def toRow(splitLine):\n",
    "    return Row(**dict(list(zip(col_names, splitLine))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(vector_data.map(toRow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is useful here for not parsing the text file each time we are calling the dataframe. The parsed results are stored in RAM on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count: double, diff_srv_rate: double, dst_bytes: double, dst_host_count: double, dst_host_diff_srv_rate: double, dst_host_rerror_rate: double, dst_host_same_src_port_rate: double, dst_host_same_srv_rate: double, dst_host_serror_rate: double, dst_host_srv_count: double, dst_host_srv_diff_host_rate: double, dst_host_srv_rerror_rate: double, dst_host_srv_serror_rate: double, duration: double, hot: double, is_guest_login: double, is_hot_login: double, land: double, logged_in: double, num_access_files: double, num_compromised: double, num_failed_logins: double, num_file_creations: double, num_outbound_cmds: double, num_root: double, num_shells: double, rerror_rate: double, root_shell: double, same_srv_rate: double, serror_rate: double, src_bytes: double, srv_count: double, srv_diff_host_rate: double, srv_rerror_rate: double, srv_serror_rate: double, su_attempted: double, urgent: double, wrong_fragment: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to compute some basic statistics on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = df.describe().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Statistics count\n",
      "num_access_files: 494021\n",
      "src_bytes: 494021\n",
      "srv_count: 494021\n",
      "num_outbound_cmds: 494021\n",
      "rerror_rate: 494021\n",
      "dst_host_srv_rerror_rate: 494021\n",
      "dst_host_same_srv_rate: 494021\n",
      "duration: 494021\n",
      "srv_rerror_rate: 494021\n",
      "srv_serror_rate: 494021\n",
      "num_file_creations: 494021\n",
      "dst_host_srv_serror_rate: 494021\n",
      "num_compromised: 494021\n",
      "is_guest_login: 494021\n",
      "dst_host_rerror_rate: 494021\n",
      "diff_srv_rate: 494021\n",
      "hot: 494021\n",
      "dst_host_srv_count: 494021\n",
      "logged_in: 494021\n",
      "is_hot_login: 494021\n",
      "num_shells: 494021\n",
      "dst_host_srv_diff_host_rate: 494021\n",
      "srv_diff_host_rate: 494021\n",
      "dst_host_same_src_port_rate: 494021\n",
      "root_shell: 494021\n",
      "su_attempted: 494021\n",
      "dst_host_count: 494021\n",
      "wrong_fragment: 494021\n",
      "count: 494021\n",
      "land: 494021\n",
      "urgent: 494021\n",
      "same_srv_rate: 494021\n",
      "num_failed_logins: 494021\n",
      "serror_rate: 494021\n",
      "summary: count\n",
      "dst_host_diff_srv_rate: 494021\n",
      "num_root: 494021\n",
      "dst_bytes: 494021\n",
      "dst_host_serror_rate: 494021\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics mean\n",
      "num_access_files: 0.001008054313480601\n",
      "src_bytes: 3025.6102959185946\n",
      "srv_count: 292.9065566038691\n",
      "num_outbound_cmds: 0.0\n",
      "rerror_rate: 0.05743340870124959\n",
      "dst_host_srv_rerror_rate: 0.057411668734728985\n",
      "dst_host_same_srv_rate: 0.7537796976247786\n",
      "duration: 47.97930249928647\n",
      "srv_rerror_rate: 0.05771894312185114\n",
      "srv_serror_rate: 0.17660880812758986\n",
      "num_file_creations: 0.0010829499150845814\n",
      "dst_host_srv_serror_rate: 0.17644262086023382\n",
      "num_compromised: 0.010212116488975164\n",
      "is_guest_login: 0.0013865807323980154\n",
      "dst_host_rerror_rate: 0.05811761038498338\n",
      "diff_srv_rate: 0.020982387388396636\n",
      "hot: 0.03451877551763994\n",
      "dst_host_srv_count: 188.66567008285074\n",
      "logged_in: 0.1482467344505598\n",
      "is_hot_login: 0.0\n",
      "num_shells: 1.0930709423283626E-4\n",
      "dst_host_srv_diff_host_rate: 0.006683501308649176\n",
      "srv_diff_host_rate: 0.028996803779597343\n",
      "dst_host_same_src_port_rate: 0.6019347558101208\n",
      "root_shell: 1.1133129968159248E-4\n",
      "su_attempted: 3.6435698077612086E-5\n",
      "dst_host_count: 232.47077755803903\n",
      "wrong_fragment: 0.0064329249161472896\n",
      "count: 332.2856902844211\n",
      "land: 4.453251987263699E-5\n",
      "urgent: 1.4169438141293589E-5\n",
      "same_srv_rate: 0.7915473431296286\n",
      "num_failed_logins: 1.5181540865671702E-4\n",
      "serror_rate: 0.17668665906914852\n",
      "summary: mean\n",
      "dst_host_diff_srv_rate: 0.030905730728006633\n",
      "num_root: 0.01135174415662492\n",
      "dst_bytes: 868.5324247349809\n",
      "dst_host_serror_rate: 0.17675396390030904\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics stddev\n",
      "num_access_files: 0.03648169000780269\n",
      "src_bytes: 988218.1010504081\n",
      "srv_count: 246.32281740435621\n",
      "num_outbound_cmds: 0.0\n",
      "rerror_rate: 0.23162347378248732\n",
      "dst_host_srv_rerror_rate: 0.23014032392212608\n",
      "dst_host_same_srv_rate: 0.4107809774268466\n",
      "duration: 707.7464723053699\n",
      "srv_rerror_rate: 0.23214698137675502\n",
      "srv_serror_rate: 0.38101658447248343\n",
      "num_file_creations: 0.09641587868539274\n",
      "dst_host_srv_serror_rate: 0.38091944805609274\n",
      "num_compromised: 1.798326257550226\n",
      "is_guest_login: 0.03721103235747115\n",
      "dst_host_rerror_rate: 0.23058950706744472\n",
      "diff_srv_rate: 0.08220549289757532\n",
      "hot: 0.7821025819213107\n",
      "dst_host_srv_count: 106.04043710192246\n",
      "logged_in: 0.3553447562178366\n",
      "is_hot_login: 0.0\n",
      "num_shells: 0.011020010195335592\n",
      "dst_host_srv_diff_host_rate: 0.0421328744158583\n",
      "srv_diff_host_rate: 0.14239746702004702\n",
      "dst_host_same_src_port_rate: 0.4813092511906108\n",
      "root_shell: 0.010550788139096308\n",
      "su_attempted: 0.007792622076193872\n",
      "dst_host_count: 64.74538037222206\n",
      "wrong_fragment: 0.13480524800509122\n",
      "count: 213.1474121303455\n",
      "land: 0.006673127217912763\n",
      "urgent: 0.005510257926739919\n",
      "same_srv_rate: 0.3881894930400765\n",
      "num_failed_logins: 0.015519596902933197\n",
      "serror_rate: 0.38071695633845914\n",
      "summary: stddev\n",
      "dst_host_diff_srv_rate: 0.10925911221797197\n",
      "num_root: 2.0127183255818193\n",
      "dst_bytes: 33040.00125210252\n",
      "dst_host_serror_rate: 0.38059309751276543\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics min\n",
      "num_access_files: 0.0\n",
      "src_bytes: 0.0\n",
      "srv_count: 0.0\n",
      "num_outbound_cmds: 0.0\n",
      "rerror_rate: 0.0\n",
      "dst_host_srv_rerror_rate: 0.0\n",
      "dst_host_same_srv_rate: 0.0\n",
      "duration: 0.0\n",
      "srv_rerror_rate: 0.0\n",
      "srv_serror_rate: 0.0\n",
      "num_file_creations: 0.0\n",
      "dst_host_srv_serror_rate: 0.0\n",
      "num_compromised: 0.0\n",
      "is_guest_login: 0.0\n",
      "dst_host_rerror_rate: 0.0\n",
      "diff_srv_rate: 0.0\n",
      "hot: 0.0\n",
      "dst_host_srv_count: 0.0\n",
      "logged_in: 0.0\n",
      "is_hot_login: 0.0\n",
      "num_shells: 0.0\n",
      "dst_host_srv_diff_host_rate: 0.0\n",
      "srv_diff_host_rate: 0.0\n",
      "dst_host_same_src_port_rate: 0.0\n",
      "root_shell: 0.0\n",
      "su_attempted: 0.0\n",
      "dst_host_count: 0.0\n",
      "wrong_fragment: 0.0\n",
      "count: 0.0\n",
      "land: 0.0\n",
      "urgent: 0.0\n",
      "same_srv_rate: 0.0\n",
      "num_failed_logins: 0.0\n",
      "serror_rate: 0.0\n",
      "summary: min\n",
      "dst_host_diff_srv_rate: 0.0\n",
      "num_root: 0.0\n",
      "dst_bytes: 0.0\n",
      "dst_host_serror_rate: 0.0\n",
      "----------------------\n",
      "----------------------\n",
      "Statistics max\n",
      "num_access_files: 8.0\n",
      "src_bytes: 6.9337564E8\n",
      "srv_count: 511.0\n",
      "num_outbound_cmds: 0.0\n",
      "rerror_rate: 1.0\n",
      "dst_host_srv_rerror_rate: 1.0\n",
      "dst_host_same_srv_rate: 1.0\n",
      "duration: 58329.0\n",
      "srv_rerror_rate: 1.0\n",
      "srv_serror_rate: 1.0\n",
      "num_file_creations: 28.0\n",
      "dst_host_srv_serror_rate: 1.0\n",
      "num_compromised: 884.0\n",
      "is_guest_login: 1.0\n",
      "dst_host_rerror_rate: 1.0\n",
      "diff_srv_rate: 1.0\n",
      "hot: 30.0\n",
      "dst_host_srv_count: 255.0\n",
      "logged_in: 1.0\n",
      "is_hot_login: 0.0\n",
      "num_shells: 2.0\n",
      "dst_host_srv_diff_host_rate: 1.0\n",
      "srv_diff_host_rate: 1.0\n",
      "dst_host_same_src_port_rate: 1.0\n",
      "root_shell: 1.0\n",
      "su_attempted: 2.0\n",
      "dst_host_count: 255.0\n",
      "wrong_fragment: 3.0\n",
      "count: 511.0\n",
      "land: 1.0\n",
      "urgent: 3.0\n",
      "same_srv_rate: 1.0\n",
      "num_failed_logins: 5.0\n",
      "serror_rate: 1.0\n",
      "summary: max\n",
      "dst_host_diff_srv_rate: 1.0\n",
      "num_root: 993.0\n",
      "dst_bytes: 5155468.0\n",
      "dst_host_serror_rate: 1.0\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in result:\n",
    "    print \"----------------------\"\n",
    "    r = l.asDict()\n",
    "    print \"Statistics {}\".format(r[\"summary\"])\n",
    "    for key in r.keys():\n",
    "        print \"{0}: {1}\".format(key, r[key])\n",
    "    print \"----------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"back.\",\"buffer_overflow.\",\"ftp_write.\",\"guess_passwd.\",\n",
    "              \"imap.\",\"ipsweep.\",\"land.\",\"loadmodule.\",\"multihop.\",\n",
    "              \"neptune.\",\"nmap.\",\"normal.\",\"perl.\",\"phf.\",\"pod.\",\"portsweep.\",\n",
    "              \"rootkit.\",\"satan.\",\"smurf.\",\"spy.\",\"teardrop.\",\"warezclient.\",\n",
    "              \"warezmaster.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_interaction_label(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # keep just numeric and logical values\n",
    "    symbolic_indexes = [1,2,3,41]\n",
    "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
    "    return [float(x) for x in clean_line_split] + [line_split[i] for i in symbolic_indexes]\n",
    "\n",
    "label_vector_data = raw_data.map(parse_interaction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_col_names = col_names + [\"protocol\", \"service\", \"flag\", \"label\"]\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "def toLabelRow(splitLine):\n",
    "    return Row(**dict(list(zip(label_col_names, splitLine))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_df = sqlContext.createDataFrame(label_vector_data.map(toLabelRow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute statistics by labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97278"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.where(label_df.label==\"normal.\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean duration by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_df.select(\"label\").groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           label|       avg(duration)|\n",
      "+----------------+--------------------+\n",
      "|    warezmaster.|               15.05|\n",
      "|          smurf.|                 0.0|\n",
      "|            pod.|                 0.0|\n",
      "|           imap.|                 6.0|\n",
      "|           nmap.|                 0.0|\n",
      "|   guess_passwd.|  2.7169811320754715|\n",
      "|        ipsweep.|0.034482758620689655|\n",
      "|      portsweep.|  1915.2990384615384|\n",
      "|          satan.|0.040276903713027064|\n",
      "|           land.|                 0.0|\n",
      "|     loadmodule.|   36.22222222222222|\n",
      "|      ftp_write.|              32.375|\n",
      "|buffer_overflow.|                91.7|\n",
      "|        rootkit.|               100.8|\n",
      "|    warezclient.|   615.2578431372549|\n",
      "|       teardrop.|                 0.0|\n",
      "|           perl.|  41.333333333333336|\n",
      "|            phf.|                 4.5|\n",
      "|       multihop.|               184.0|\n",
      "|        neptune.|                 0.0|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df.select(\"label\", \"duration\").groupBy(\"label\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Machine learning with Apache Spark\n",
    "Now that the inputs are defined, we can apply some basics (or advanced) data processing functions to classify the type of interactions (i.e. \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "s = StringIndexer(inputCol=\"label\", outputCol=\"idx_label\").fit(label_df.select(col_names + [\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = s.transform(label_df.select(col_names + [\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = label_df.select(col_names + [\"label\"]).randomSplit([0.6,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy on both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|prediction|idx_label| count|\n",
      "+----------+---------+------+\n",
      "|       1.0|     12.0|    13|\n",
      "|       1.0|     19.0|     4|\n",
      "|       1.0|     16.0|     5|\n",
      "|       1.0|     17.0|     5|\n",
      "|       1.0|     15.0|     3|\n",
      "|       1.0|     11.0|    29|\n",
      "|       1.0|      1.0| 42881|\n",
      "|       1.0|     14.0|     6|\n",
      "|       1.0|     18.0|     2|\n",
      "|       1.0|      6.0|   396|\n",
      "|       1.0|      8.0|   403|\n",
      "|       1.0|     10.0|    92|\n",
      "|       1.0|     13.0|     8|\n",
      "|       1.0|      0.0|  1843|\n",
      "|       1.0|     20.0|     1|\n",
      "|       1.0|      4.0|   640|\n",
      "|       1.0|      5.0|   472|\n",
      "|       1.0|     22.0|     2|\n",
      "|       1.0|      2.0| 39127|\n",
      "|       0.0|      0.0|110503|\n",
      "+----------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153384\n"
     ]
    }
   ],
   "source": [
    "preds = model.transform(test)\n",
    "print preds.where(preds.prediction == preds.idx_label).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try applying a PCA before learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, pca, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.transform(test).select(\"prediction\", \"idx_label\").groupBy(\"prediction\", \"idx_label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try applying a kmeans to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=2, seed=1, featuresCol=\"features\", predictionCol=\"kmeans_pred\")\n",
    "assemblor = VectorAssembler(inputCols=col_names, outputCol=\"features\")\n",
    "kmeans_assemblor = VectorAssembler(inputCols=col_names+[\"kmeans_pred\"], outputCol=\"kmeans_features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"kmeans_features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, assemblor, kmeans, kmeans_assemblor, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-a18dbbe2e9bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/pyspark/ml/pipeline.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \"\"\"\n\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mlegoff/Workspace/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/pypilok/softs/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "train, test = label_df.select(col_names + [\"protocol\"] + [\"label\"]).randomSplit([0.6,0.4])\n",
    "protocols = StringIndexer(inputCol=\"protocol\", outputCol=\"idx_protocol\")\n",
    "ohe_protocol = OneHotEncoder(inputCol=\"idx_protocol\", outputCol=\"ohe_protocol\")\n",
    "assemblor = VectorAssembler(inputCols=col_names+[\"ohe_protocol\"], outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"idx_label\", maxDepth=1, maxBins=32, numTrees=1)\n",
    "pipeline = Pipeline(stages=[s, protocols, ohe_protocol, assemblor, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World record is at 95%. Show what you've got..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
